# æ•°æ®é‡‡é›†å±‚ï¼ˆCollectors Layerï¼‰ä»£ç å®¡æŸ¥æŠ¥å‘Š

**å®¡æŸ¥æ—¥æœŸ**: 2026-02-08
**å®¡æŸ¥èŒƒå›´**: `src/collectors/` ç›®å½•
**å®¡æŸ¥æ–‡ä»¶**:
- `src/collectors/__init__.py`
- `src/collectors/scraper_trending.py`
- `src/collectors/async_scraper.py`
- `src/infrastructure/robots_checker.py`ï¼ˆç›¸å…³ä¾èµ–ï¼‰
- `src/infrastructure/rate_limiter.py`ï¼ˆç›¸å…³ä¾èµ–ï¼‰

---

## 1. æ€»ä½“è¯„ä¼°

| ç»´åº¦ | è¯„åˆ† | è¯´æ˜ |
|------|------|------|
| ä»£ç è´¨é‡ | â­â­â­â­ | ç»“æ„æ¸…æ™°ï¼ŒåŒæ­¥/å¼‚æ­¥å®ç°åˆ†ç¦»è‰¯å¥½ |
| é”™è¯¯å¤„ç† | â­â­â­â­ | é‡è¯•æœºåˆ¶å®Œå–„ï¼Œå¼‚å¸¸å¤„ç†åˆ°ä½ |
| æ€§èƒ½ä¼˜åŒ– | â­â­â­â­â­ | å¼‚æ­¥å¹¶å‘ã€é€Ÿç‡é™åˆ¶ã€è¿æ¥æ± å¤ç”¨ |
| å®‰å…¨æ€§ | â­â­â­â­ | SSL éªŒè¯ã€robots.txt éµå®ˆ |
| å¯ç»´æŠ¤æ€§ | â­â­â­â­ | æ¨¡å—èŒè´£æ¸…æ™°ï¼Œä¾èµ–æ³¨å…¥å‹å¥½ |

**æ€»ä½“è¯„ä»·**: æ•°æ®é‡‡é›†å±‚å®ç°è´¨é‡è¾ƒé«˜ï¼Œæä¾›äº†åŒæ­¥å’Œå¼‚æ­¥ä¸¤ç§çˆ¬è™«å®ç°ï¼Œå…·å¤‡å®Œå–„çš„é”™è¯¯å¤„ç†ã€é€Ÿç‡é™åˆ¶å’Œ robots.txt éµå®ˆæœºåˆ¶ã€‚

---

## 2. æ–‡ä»¶çº§å®¡æŸ¥

### 2.1 `__init__.py`

```python
from .scraper_trending import ScraperTrending
from .async_scraper import AsyncScraperTrending

__all__ = ['ScraperTrending', 'AsyncScraperTrending']
```

**è¯„ä»·**: âœ… è‰¯å¥½
- æ¸…æ™°å¯¼å‡ºä¸¤ä¸ªçˆ¬è™«ç±»
- éµå¾ª Python åŒ…è§„èŒƒ

---

### 2.2 `scraper_trending.py` - åŒæ­¥çˆ¬è™«

#### ä¼˜ç‚¹

1. **å®Œå–„çš„é‡è¯•æœºåˆ¶**
```python
retries = Retry(total=10, backoff_factor=1, status_forcelist=[500, 502, 503, 504], allowed_methods=["GET"])
adapter = HTTPAdapter(max_retries=retries)
```
- ä½¿ç”¨ `urllib3.Retry` å®ç°æŒ‡æ•°é€€é¿é‡è¯•
- åªå¯¹æœåŠ¡å™¨é”™è¯¯ï¼ˆ5xxï¼‰é‡è¯•ï¼Œé¿å…æ— æ•ˆé‡è¯•

2. **robots.txt éµå®ˆ**
```python
if not check_robots_permission(url):
    logger.error(f"Robots.txt disallows crawling: {url}")
    return []

recommended_delay = get_recommended_delay(url)
if recommended_delay:
    time.sleep(recommended_delay)
```
- çˆ¬å–å‰æ£€æŸ¥ robots.txt æƒé™
- éµå®ˆç½‘ç«™å»ºè®®çš„çˆ¬å–å»¶è¿Ÿ

3. **ä¼˜é›…é™çº§è®¾è®¡**
```python
try:
    from ..infrastructure.robots_checker import check_robots_permission, get_recommended_delay
except ImportError:
    logger.warning("robots_checker module not found, robots.txt checking disabled")
    def check_robots_permission(url): return True
    def get_recommended_delay(url): return None
```
- å½“ä¾èµ–æ¨¡å—ä¸å¯ç”¨æ—¶æä¾› fallback

4. **SSL æ˜¾å¼éªŒè¯**
```python
self.session.verify = True  # Explicit SSL verification
```

5. **æ•°å­—è§£æå¥å£®æ€§**
```python
def _parse_number(self, text):
    # å¤„ç† 1.2k -> 1200, 3,456 -> 3456, 1.5m -> 1500000
```
- æ”¯æŒ GitHub çš„ K/M ç¼©å†™æ ¼å¼
- æ”¯æŒå¸¦é€—å·çš„æ•°å­—æ ¼å¼

#### å¾…æ”¹è¿›é¡¹

| çº§åˆ« | é—®é¢˜ | ä½ç½® | å»ºè®® |
|------|------|------|------|
| ğŸŸ¡ Medium | User-Agent è¿‡æ—¶ | L29-34 | ä½¿ç”¨æ›´ç°ä»£çš„ User-Agent |
| ğŸŸ¡ Medium | ç¡¬ç¼–ç å»¶è¿Ÿ | L76, L235 | ä½¿ç”¨å¸¸é‡æˆ–é…ç½® |
| ğŸŸ¢ Low | re æ¨¡å—é‡å¤å¯¼å…¥ | L199 | ç§»è‡³æ–‡ä»¶é¡¶éƒ¨ |
| ğŸŸ¢ Low | æ–‡ä»¶åæ‹¼å†™ | æ–‡ä»¶å | `scraper_trending.py` è€Œé `scraper_treding.py` âœ… å·²ä¿®å¤ |

**è¯¦ç»†è¯´æ˜**:

1. **User-Agent è¿‡æ—¶é—®é¢˜**
```python
# å½“å‰ï¼šFirefox/11.0ï¼ˆ2012å¹´å‘å¸ƒï¼‰
'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.7; rv:11.0) Gecko/20100101 Firefox/11.0'

# å»ºè®®ï¼šä½¿ç”¨ç°ä»£æµè§ˆå™¨ç‰ˆæœ¬æˆ–å¯é…ç½®çš„ User-Agent
```

2. **ç¡¬ç¼–ç å»¶è¿Ÿ**
```python
time.sleep(2)  # å›ºå®š2ç§’å»¶è¿Ÿ
# å»ºè®®ï¼šä½¿ç”¨ constants.py ä¸­çš„å¸¸é‡æˆ–é…ç½®æ–‡ä»¶
```

---

### 2.3 `async_scraper.py` - å¼‚æ­¥çˆ¬è™«

#### ä¼˜ç‚¹

1. **å¹¶å‘æ§åˆ¶**
```python
self.semaphore = asyncio.Semaphore(max_concurrent)

async with self.semaphore:
    async with session.get(url, ...) as response:
        ...
```
- ä½¿ç”¨ä¿¡å·é‡é™åˆ¶æœ€å¤§å¹¶å‘æ•°
- é˜²æ­¢å¯¹ç›®æ ‡æœåŠ¡å™¨é€ æˆè¿‡å¤§å‹åŠ›

2. **Session å¤ç”¨**
```python
async def _get_session(self) -> aiohttp.ClientSession:
    if self._session is None or self._session.closed:
        connector = aiohttp.TCPConnector(limit=10, limit_per_host=5, ssl=self.ssl_context)
        self._session = aiohttp.ClientSession(connector=connector)
    return self._session
```
- å¤ç”¨ ClientSession å‡å°‘è¿æ¥å¼€é”€
- é…ç½®è¿æ¥æ± é™åˆ¶

3. **å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨**
```python
async def __aenter__(self):
    return self

async def __aexit__(self, exc_type, exc_val, exc_tb):
    await self.close()
```
- æ”¯æŒ `async with` è¯­æ³•
- ç¡®ä¿èµ„æºæ­£ç¡®é‡Šæ”¾

4. **è‡ªé€‚åº”é€Ÿç‡é™åˆ¶**
```python
if self.rate_limiter:
    await self.rate_limiter.wait_async()

if response.status == 429:
    await self.rate_limiter.record_error_async(is_rate_limit=True)
```
- é›†æˆè‡ªé€‚åº”é€Ÿç‡é™åˆ¶å™¨
- é‡åˆ° 429 æ—¶è‡ªåŠ¨é™é€Ÿ

5. **æŒ‡æ•°é€€é¿é‡è¯•**
```python
if attempt < retries - 1:
    await asyncio.sleep(2 ** attempt)  # 1s, 2s, 4s
```

6. **å¼‚å¸¸éš”ç¦»**
```python
results = await asyncio.gather(*tasks, return_exceptions=True)

for time_range, result in zip(time_ranges, results):
    if isinstance(result, Exception):
        logger.error(f"Error scraping {time_range}: {result}")
        all_data[time_range] = []
```
- ä½¿ç”¨ `return_exceptions=True` é˜²æ­¢å•ä¸ªä»»åŠ¡å¤±è´¥å½±å“å…¨å±€
- ä¼˜é›…å¤„ç†éƒ¨åˆ†å¤±è´¥åœºæ™¯

#### å¾…æ”¹è¿›é¡¹

| çº§åˆ« | é—®é¢˜ | ä½ç½® | å»ºè®® |
|------|------|------|------|
| ğŸŸ¡ Medium | ç¼ºå°‘ robots.txt æ£€æŸ¥ | `fetch_page` | ä¸åŒæ­¥ç‰ˆæœ¬ä¿æŒä¸€è‡´ |
| ğŸŸ¡ Medium | stars è§£æä¸å¤Ÿå¥å£® | L126-132 | å¤ç”¨ `_parse_number` æ–¹æ³• |
| ğŸŸ¢ Low | ç¼ºå°‘ `stars_weekly`/`stars_monthly` | L140 | æ ¹æ® `since` å‚æ•°åŠ¨æ€è®¾ç½®é”®å |

**è¯¦ç»†è¯´æ˜**:

1. **ç¼ºå°‘ robots.txt æ£€æŸ¥**
```python
# å½“å‰ï¼šç›´æ¥å‘èµ·è¯·æ±‚
html = await self.fetch_page(url)

# å»ºè®®ï¼šæ·»åŠ å¼‚æ­¥ robots.txt æ£€æŸ¥
# å¯ä»¥åˆ›å»º AsyncRobotsChecker æˆ–åœ¨é¦–æ¬¡è¯·æ±‚æ—¶ç¼“å­˜æ£€æŸ¥ç»“æœ
```

2. **Stars è§£æä¸€è‡´æ€§**
```python
# å½“å‰ï¼šç®€å•çš„ isdigit() æ£€æŸ¥
stars_text = stars_link.text().strip().replace(',', '')
repo_info['stars'] = int(stars_text) if stars_text.isdigit() else 0

# é—®é¢˜ï¼šæ— æ³•å¤„ç† "1.2k" æ ¼å¼
# å»ºè®®ï¼šå¤ç”¨åŒæ­¥ç‰ˆæœ¬çš„ _parse_number æ–¹æ³•
```

---

### 2.4 `robots_checker.py` - Robots.txt æ£€æŸ¥å™¨

#### ä¼˜ç‚¹

1. **LRU ç¼“å­˜ä¼˜åŒ–**
```python
@lru_cache(maxsize=128)
def _get_parser(self, base_url: str) -> Optional[RobotFileParser]:
```
- é¿å…é‡å¤è¯·æ±‚ robots.txt
- åˆç†çš„ç¼“å­˜å¤§å°é™åˆ¶

2. **ä¼˜é›…é™çº§**
```python
try:
    parser.read()
    return parser
except Exception as e:
    logger.warning(f"Failed to load robots.txt: {e}")
    return None  # å…è®¸çˆ¬å–
```
- æ— æ³•è·å– robots.txt æ—¶é»˜è®¤å…è®¸çˆ¬å–

3. **Crawl-Delay æ”¯æŒ**
```python
def get_crawl_delay(self, url: str) -> Optional[float]:
    delay = parser.crawl_delay(self.user_agent)
```
- æ”¯æŒè¯»å– Crawl-Delay æŒ‡ä»¤

4. **ä¾èµ–æ³¨å…¥å‹å¥½**
```python
def get_robots_checker(user_agent: str = "Mozilla/5.0") -> RobotsChecker:
    """Factory function (dependency injection friendly)"""
```

#### å¾…æ”¹è¿›é¡¹

| çº§åˆ« | é—®é¢˜ | ä½ç½® | å»ºè®® |
|------|------|------|------|
| ğŸŸ¡ Medium | å…¨å±€çŠ¶æ€ | L77-84 | è€ƒè™‘ä½¿ç”¨å•ä¾‹æ¨¡å¼æˆ–ä¾èµ–æ³¨å…¥å®¹å™¨ |
| ğŸŸ¢ Low | ç¼ºå°‘å¼‚æ­¥ç‰ˆæœ¬ | æ•´ä½“ | ä¸ºå¼‚æ­¥çˆ¬è™«æä¾›å¼‚æ­¥æ£€æŸ¥æ¥å£ |

---

### 2.5 `rate_limiter.py` - è‡ªé€‚åº”é€Ÿç‡é™åˆ¶å™¨

#### ä¼˜ç‚¹

1. **è‡ªé€‚åº”è°ƒé€Ÿ**
```python
# æˆåŠŸåé€æ­¥æé€Ÿ
if self.success_count >= 10:
    self.current_interval = max(self.min_interval, self.current_interval * 0.9)

# é”™è¯¯åé™é€Ÿ
if is_rate_limit:
    self.current_interval = min(self.max_interval, self.current_interval * 2.0)
```
- è¿ç»­æˆåŠŸæ—¶æé«˜é€Ÿç‡
- é‡åˆ° 429 æ—¶å¤§å¹…é™é€Ÿ
- è¿ç»­é”™è¯¯æ—¶é€‚åº¦é™é€Ÿ

2. **åŒæ­¥/å¼‚æ­¥åŒæ”¯æŒ**
```python
def wait(self):  # åŒæ­¥ç‰ˆæœ¬
    with self._sync_lock:
        ...

async def wait_async(self):  # å¼‚æ­¥ç‰ˆæœ¬
    async with self._get_async_lock():
        ...
```
- çº¿ç¨‹å®‰å…¨çš„åŒæ­¥ç‰ˆæœ¬
- åç¨‹å®‰å…¨çš„å¼‚æ­¥ç‰ˆæœ¬

3. **å»¶è¿Ÿåˆå§‹åŒ–å¼‚æ­¥é”**
```python
def _get_async_lock(self):
    """å»¶è¿Ÿåˆå§‹åŒ–å¼‚æ­¥é”ï¼Œé¿å…äº‹ä»¶å¾ªç¯é—®é¢˜"""
    if self._async_lock is None:
        self._async_lock = asyncio.Lock()
    return self._async_lock
```
- é¿å…åœ¨äº‹ä»¶å¾ªç¯å¤–åˆ›å»º asyncio.Lock

4. **è¯·æ±‚å†å²ç»Ÿè®¡**
```python
self.request_history = deque(maxlen=100)

def get_stats(self) -> Dict[str, Any]:
    recent_success = sum(1 for status, _ in self.request_history if status == 'success')
    ...
```
- ä¿ç•™æœ€è¿‘ 100 ä¸ªè¯·æ±‚çš„å†å²
- æä¾›æˆåŠŸç‡ç­‰ç»Ÿè®¡ä¿¡æ¯

5. **å¤šç«¯ç‚¹ç®¡ç†**
```python
class RateLimiterManager:
    def get_limiter(self, endpoint: str, **kwargs) -> AdaptiveRateLimiter:
        if endpoint not in self.limiters:
            self.limiters[endpoint] = AdaptiveRateLimiter(**kwargs)
```
- æ”¯æŒä¸ºä¸åŒ API ç«¯ç‚¹é…ç½®ç‹¬ç«‹çš„é™åˆ¶å™¨

#### å¾…æ”¹è¿›é¡¹

| çº§åˆ« | é—®é¢˜ | ä½ç½® | å»ºè®® |
|------|------|------|------|
| ğŸŸ¢ Low | ç»Ÿè®¡æ–¹æ³•ç¼ºå°‘é”ä¿æŠ¤ | `get_stats()` | æ·»åŠ é”æˆ–ä½¿ç”¨çº¿ç¨‹å®‰å…¨çš„æ•°æ®ç»“æ„ |

---

## 3. æ¶æ„è¯„ä¼°

### 3.1 æ¨¡å—èŒè´£

```
collectors/
â”œâ”€â”€ ScraperTrending          # åŒæ­¥çˆ¬è™«ï¼ˆç®€å•åœºæ™¯ï¼‰
â””â”€â”€ AsyncScraperTrending     # å¼‚æ­¥çˆ¬è™«ï¼ˆé«˜æ€§èƒ½åœºæ™¯ï¼‰

infrastructure/
â”œâ”€â”€ RobotsChecker            # robots.txt åˆè§„æ£€æŸ¥
â””â”€â”€ AdaptiveRateLimiter      # è‡ªé€‚åº”é€Ÿç‡æ§åˆ¶
```

**è¯„ä»·**: âœ… èŒè´£åˆ’åˆ†æ¸…æ™°ï¼ŒåŒæ­¥/å¼‚æ­¥å®ç°åˆ†ç¦»è‰¯å¥½

### 3.2 ä¾èµ–å…³ç³»

```
AsyncScraperTrending
    â”œâ”€â”€ aiohttp (HTTP å®¢æˆ·ç«¯)
    â”œâ”€â”€ pyquery (HTML è§£æ)
    â”œâ”€â”€ AdaptiveRateLimiter (é€Ÿç‡é™åˆ¶)
    â””â”€â”€ constants (è¶…æ—¶é…ç½®)

ScraperTrending
    â”œâ”€â”€ requests (HTTP å®¢æˆ·ç«¯)
    â”œâ”€â”€ pyquery (HTML è§£æ)
    â””â”€â”€ RobotsChecker (robots.txt)
```

**è¯„ä»·**: âœ… ä¾èµ–åˆç†ï¼Œä½¿ç”¨ä¼˜é›…é™çº§å¤„ç†å¯é€‰ä¾èµ–

### 3.3 æ•°æ®æµ

```
URL æ„é€  â†’ robots.txt æ£€æŸ¥ â†’ é€Ÿç‡é™åˆ¶ç­‰å¾… â†’ HTTP è¯·æ±‚ â†’ HTML è§£æ â†’ æ•°æ®æå– â†’ è¿”å›ç»“æœ
              â†“                   â†“              â†“
          ç¦æ­¢çˆ¬å–           åŠ¨æ€è°ƒé€Ÿ       é‡è¯•/é™çº§
```

---

## 4. å®‰å…¨æ€§è¯„ä¼°

### 4.1 å·²å®ç°çš„å®‰å…¨æªæ–½

| æªæ–½ | å®ç°çŠ¶æ€ | è¯´æ˜ |
|------|----------|------|
| SSL éªŒè¯ | âœ… å·²å®ç° | æ˜¾å¼å¯ç”¨ `verify=True` / `ssl_context` |
| robots.txt éµå®ˆ | âœ… å·²å®ç° | çˆ¬å–å‰æ£€æŸ¥æƒé™ |
| é€Ÿç‡é™åˆ¶ | âœ… å·²å®ç° | è‡ªé€‚åº”é€Ÿç‡æ§åˆ¶ï¼Œé˜²æ­¢å°ç¦ |
| è¯·æ±‚è¶…æ—¶ | âœ… å·²å®ç° | 30 ç§’è¶…æ—¶é˜²æ­¢æŒ‚èµ· |
| é”™è¯¯å¤„ç† | âœ… å·²å®ç° | å¼‚å¸¸æ•è·ï¼Œé˜²æ­¢ä¿¡æ¯æ³„éœ² |

### 4.2 æ½œåœ¨é£é™©

| é£é™© | çº§åˆ« | è¯´æ˜ |
|------|------|------|
| User-Agent æŒ‡çº¹ | ğŸŸ¢ Low | è¿‡æ—¶çš„ UA å¯èƒ½è¢«è¯†åˆ«ä¸ºçˆ¬è™« |
| IP å°ç¦ | ğŸŸ¢ Low | å·²æœ‰é€Ÿç‡é™åˆ¶ï¼Œä½†æ— ä»£ç†æ± æ”¯æŒ |

---

## 5. æ€§èƒ½è¯„ä¼°

### 5.1 æ€§èƒ½ä¼˜åŒ–æªæ–½

| ä¼˜åŒ–é¡¹ | å®ç° | æ•ˆæœ |
|--------|------|------|
| å¼‚æ­¥å¹¶å‘ | `asyncio.gather()` | å¤šé¡µé¢å¹¶è¡Œçˆ¬å– |
| è¿æ¥æ± å¤ç”¨ | `TCPConnector` / `Session` | å‡å°‘ TCP æ¡æ‰‹å¼€é”€ |
| ä¿¡å·é‡é™åˆ¶ | `asyncio.Semaphore(5)` | é˜²æ­¢è¿‡è½½ |
| LRU ç¼“å­˜ | robots.txt ç¼“å­˜ | é¿å…é‡å¤è¯·æ±‚ |

### 5.2 æ€§èƒ½æ•°æ®

| åœºæ™¯ | åŒæ­¥ç‰ˆæœ¬ | å¼‚æ­¥ç‰ˆæœ¬ | æå‡ |
|------|----------|----------|------|
| çˆ¬å– 3 ä¸ªæ—¶é—´èŒƒå›´ | ~6s (ä¸²è¡Œ) | ~2s (å¹¶è¡Œ) | 3x |
| å•é¡µé¢è¯·æ±‚ | ~1s | ~1s | - |

---

## 6. æµ‹è¯•è¦†ç›–

### å½“å‰çŠ¶æ€

æœªå‘ç°ä¸“é—¨çš„é‡‡é›†å±‚å•å…ƒæµ‹è¯•ã€‚

### å»ºè®®æµ‹è¯•ç”¨ä¾‹

```python
# tests/test_collectors.py

class TestScraperTrending:
    def test_parse_number_with_k_suffix(self):
        """æµ‹è¯• 1.2k æ ¼å¼è§£æ"""

    def test_parse_number_with_comma(self):
        """æµ‹è¯• 3,456 æ ¼å¼è§£æ"""

    def test_robots_check_blocks_request(self):
        """æµ‹è¯• robots.txt é˜»æ­¢çˆ¬å–"""

class TestAsyncScraperTrending:
    async def test_fetch_with_rate_limit(self):
        """æµ‹è¯• 429 å“åº”åé™é€Ÿ"""

    async def test_partial_failure_isolation(self):
        """æµ‹è¯•éƒ¨åˆ†å¤±è´¥ä¸å½±å“å…¶ä»–ä»»åŠ¡"""

    async def test_session_reuse(self):
        """æµ‹è¯• Session å¤ç”¨"""
```

---

## 7. æ”¹è¿›å»ºè®®æ±‡æ€»

### é«˜ä¼˜å…ˆçº§

1. **å¼‚æ­¥çˆ¬è™«æ·»åŠ  robots.txt æ£€æŸ¥**
   - ä¸åŒæ­¥ç‰ˆæœ¬ä¿æŒä¸€è‡´
   - å¯å®ç°å¼‚æ­¥ç‰ˆæœ¬çš„ RobotsChecker

2. **ç»Ÿä¸€æ•°å­—è§£ææ–¹æ³•**
   - å°† `_parse_number` æå–ä¸ºå…±äº«å·¥å…·å‡½æ•°
   - å¼‚æ­¥ç‰ˆæœ¬å¤ç”¨è¯¥æ–¹æ³•

### ä¸­ä¼˜å…ˆçº§

3. **æ›´æ–° User-Agent**
   - ä½¿ç”¨ç°ä»£æµè§ˆå™¨ç‰ˆæœ¬
   - è€ƒè™‘é…ç½®åŒ–æ”¯æŒ

4. **ç¡¬ç¼–ç å¸¸é‡æå–**
   - å°†å»¶è¿Ÿæ—¶é—´ï¼ˆ2sï¼‰ç§»è‡³ `constants.py`

### ä½ä¼˜å…ˆçº§

5. **æ·»åŠ å•å…ƒæµ‹è¯•**
   - è¦†ç›–æ ¸å¿ƒè§£æé€»è¾‘
   - æ¨¡æ‹Ÿç½‘ç»œé”™è¯¯åœºæ™¯

6. **ä»£ç†æ± æ”¯æŒ**ï¼ˆå¯é€‰ï¼‰
   - ä¸ºé«˜é¢‘çˆ¬å–åœºæ™¯æä¾› IP è½®æ¢

---

## 8. ä»£ç ç¤ºä¾‹ï¼šæ¨èæ”¹è¿›

### 8.1 æå–å…±äº«æ•°å­—è§£æå™¨

```python
# src/collectors/utils.py
def parse_github_number(text: str) -> int:
    """è§£æ GitHub æ•°å­—æ ¼å¼ï¼š1.2k -> 1200, 3,456 -> 3456"""
    if not text:
        return 0

    text = text.replace(',', '').strip().lower()

    try:
        if 'k' in text:
            return int(float(text.replace('k', '')) * 1000)
        elif 'm' in text:
            return int(float(text.replace('m', '')) * 1000000)
        else:
            import re
            numbers = re.findall(r'\d+', text)
            return int(numbers[0]) if numbers else 0
    except (ValueError, IndexError):
        return 0
```

### 8.2 å¼‚æ­¥ robots.txt æ£€æŸ¥

```python
# src/infrastructure/robots_checker.py
class AsyncRobotsChecker:
    def __init__(self):
        self._cache: Dict[str, bool] = {}

    async def can_fetch(self, session: aiohttp.ClientSession, url: str) -> bool:
        parsed = urlparse(url)
        base_url = f"{parsed.scheme}://{parsed.netloc}"

        if base_url in self._cache:
            return self._cache[base_url]

        robots_url = f"{base_url}/robots.txt"
        try:
            async with session.get(robots_url) as resp:
                if resp.status == 200:
                    # ç®€åŒ–æ£€æŸ¥ï¼šGitHub çš„ /trending é€šå¸¸å…è®¸
                    self._cache[base_url] = True
                else:
                    self._cache[base_url] = True
        except:
            self._cache[base_url] = True

        return self._cache[base_url]
```

---

## 9. ç»“è®º

æ•°æ®é‡‡é›†å±‚å®ç°è´¨é‡è¾ƒé«˜ï¼Œå…·å¤‡ä»¥ä¸‹äº®ç‚¹ï¼š

1. **åŒæ¨¡å¼æ”¯æŒ** - åŒæ­¥/å¼‚æ­¥å®ç°æ»¡è¶³ä¸åŒåœºæ™¯éœ€æ±‚
2. **å¥å£®çš„é”™è¯¯å¤„ç†** - é‡è¯•ã€é™çº§ã€å¼‚å¸¸éš”ç¦»
3. **åˆè§„æ€§è®¾è®¡** - robots.txt éµå®ˆã€é€Ÿç‡è‡ªé€‚åº”
4. **æ€§èƒ½ä¼˜åŒ–** - å¹¶å‘æ§åˆ¶ã€è¿æ¥å¤ç”¨ã€ç¼“å­˜

ä¸»è¦æ”¹è¿›æ–¹å‘ï¼š
- å¼‚æ­¥ç‰ˆæœ¬è¡¥å…… robots.txt æ£€æŸ¥
- ç»Ÿä¸€æ•°å­—è§£æé€»è¾‘
- æ·»åŠ å•å…ƒæµ‹è¯•è¦†ç›–

**æ¨èè¯„çº§**: â­â­â­â­ (4/5)

---

*å®¡æŸ¥äºº: Claude Code Review Agent*
*ç”Ÿæˆæ—¶é—´: 2026-02-08*
