"""
GitHubé¡¹ç›®æ·±åº¦åˆ†ææ¨¡å— - æä¾›è¯¦ç»†çš„æŠ€æœ¯åˆ†ææŠ¥å‘Š
"""

import re
import base64
import asyncio
import httpx
from loguru import logger
from typing import Any, Dict, Optional


class DeepAnalyzer:
    """GitHubé¡¹ç›®æ·±åº¦åˆ†æå™¨ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰"""

    def __init__(self, github_token: Optional[str] = None):
        self.github_token = github_token
        self._client: Optional[httpx.AsyncClient] = None
        self._lock = asyncio.Lock()
        self._headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'application/vnd.github.v3+json'
        }
        if github_token:
            self._headers['Authorization'] = f'token {github_token}'

    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        await self._get_client()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨é€€å‡ºï¼Œç¡®ä¿èµ„æºæ¸…ç†"""
        await self.close()
        return False

    async def _get_client(self) -> httpx.AsyncClient:
        """è·å–æˆ–åˆ›å»º httpx å¼‚æ­¥å®¢æˆ·ç«¯"""
        async with self._lock:
            if self._client is None or self._client.is_closed:
                self._client = httpx.AsyncClient(headers=self._headers, timeout=10.0)
            return self._client

    async def close(self):
        """å…³é—­å®¢æˆ·ç«¯é‡Šæ”¾èµ„æº"""
        if self._client and not self._client.is_closed:
            await self._client.aclose()
            self._client = None

    async def fetch_readme(self, repo_url: str, max_lines: int = 500) -> Optional[str]:
        """æŠ“å–é¡¹ç›®READMEå†…å®¹ï¼ˆå‰Nè¡Œï¼‰"""
        try:
            owner, repo = self._parse_repo_url(repo_url)
            if not owner or not repo:
                logger.warning(f"Invalid repository URL: {repo_url}")
                return None

            client = await self._get_client()
            api_url = f"https://api.github.com/repos/{owner}/{repo}/readme"
            response = await client.get(api_url)

            remaining = response.headers.get('X-RateLimit-Remaining')
            if remaining and int(remaining) < 10:
                reset_time = response.headers.get('X-RateLimit-Reset')
                logger.warning(f"GitHub API rate limit low: {remaining} requests remaining, resets at {reset_time}")

            if response.status_code == 403:
                logger.error(f"GitHub API rate limit exceeded or access forbidden for {owner}/{repo}")
                return None

            if response.status_code == 404:
                logger.warning(f"README not found for {owner}/{repo}")
                return None

            response.raise_for_status()
            readme_data = response.json()

            readme_content = base64.b64decode(readme_data['content']).decode('utf-8')
            lines = readme_content.split('\n')[:max_lines]
            readme_text = '\n'.join(lines)

            logger.info(f"Successfully fetched README for {owner}/{repo} ({len(lines)} lines)")
            return readme_text

        except httpx.RequestError as e:
            logger.error(f"Failed to fetch README for {repo_url}: {e}")
            return None
        except Exception as e:
            logger.error(f"Error processing README: {e}")
            return None

    def extract_dependencies(self, readme_content: Optional[str]) -> list:
        """ä»READMEä¸­è¯†åˆ«çŸ¥ååº“å’ŒæŠ€æœ¯æ ˆ"""
        if not readme_content:
            return []

        known_libraries = {
            'Python': ['django', 'flask', 'fastapi', 'pytorch', 'tensorflow', 'numpy', 'pandas', 'scikit-learn', 'keras', 'sqlalchemy', 'celery', 'redis', 'asyncio'],
            'JavaScript': ['react', 'vue', 'angular', 'next.js', 'express', 'nest.js', 'webpack', 'vite', 'typescript', 'node.js', 'electron'],
            'Go': ['gin', 'echo', 'fiber', 'gorm', 'cobra', 'kubernetes', 'docker', 'prometheus'],
            'Java': ['spring', 'spring boot', 'hibernate', 'mybatis', 'maven', 'gradle', 'kafka', 'elasticsearch'],
            'Rust': ['tokio', 'actix', 'rocket', 'serde', 'diesel', 'wasm'],
            'Database': ['postgresql', 'mysql', 'mongodb', 'sqlite', 'redis', 'cassandra'],
            'DevOps': ['docker', 'kubernetes', 'jenkins', 'github actions', 'terraform', 'ansible'],
            'AI/ML': ['openai', 'langchain', 'llama', 'hugging face', 'stable diffusion', 'transformers']
        }

        found_deps = []
        readme_lower = readme_content.lower()

        for category, libs in known_libraries.items():
            for lib in libs:
                if lib.lower() in readme_lower:
                    found_deps.append({'name': lib, 'category': category})

        unique_deps = []
        seen = set()
        for dep in found_deps:
            if dep['name'] not in seen:
                unique_deps.append(dep)
                seen.add(dep['name'])

        return unique_deps[:10]

    def build_deep_analysis_prompt(self, repo_data: Dict[str, Any], readme_content: Optional[str], dependencies: list) -> str:
        """æ„å»ºæ·±åº¦åˆ†ææç¤ºè¯"""
        deps_text = ', '.join([f"{d['name']} ({d['category']})" for d in dependencies]) if dependencies else "æœªè¯†åˆ«åˆ°ä¸»æµæŠ€æœ¯æ ˆ"
        readme_excerpt = readme_content[:2000] if readme_content else "æ— READMEå†…å®¹"

        prompt = f"""ä½ æ˜¯èµ„æ·±æŠ€æœ¯æ¶æ„å¸ˆå’ŒGitHubå¼€æºé¡¹ç›®åˆ†æä¸“å®¶ã€‚è¯·å¯¹ä»¥ä¸‹é¡¹ç›®è¿›è¡Œæ·±åº¦æŠ€æœ¯åˆ†æï¼š

**é¡¹ç›®åŸºæœ¬ä¿¡æ¯ï¼š**
- åç§°: {repo_data.get('name', 'Unknown')}
- æè¿°: {repo_data.get('description', 'No description')}
- Stars: {repo_data.get('stars', 0)}
- è¯­è¨€: {repo_data.get('language', 'Unknown')}
- URL: {repo_data.get('url', '')}

**è¯†åˆ«åˆ°çš„æŠ€æœ¯æ ˆï¼š**
{deps_text}

**READMEæ‘˜è¦ï¼š**
```
{readme_excerpt}
```

**è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºæ·±åº¦åˆ†ææŠ¥å‘Šï¼ˆä½¿ç”¨ä¸­æ–‡ï¼‰ï¼š**

### ğŸ—ï¸ æŠ€æœ¯æ¶æ„
[åˆ†æé¡¹ç›®çš„æŠ€æœ¯æ¶æ„è®¾è®¡ï¼ŒåŒ…æ‹¬æ ¸å¿ƒç»„ä»¶ã€è®¾è®¡æ¨¡å¼ã€æŠ€æœ¯é€‰å‹çš„åˆç†æ€§]

### âœ¨ æŠ€æœ¯åˆ›æ–°ç‚¹
[æŒ‡å‡ºé¡¹ç›®çš„æŠ€æœ¯åˆ›æ–°ä¹‹å¤„ï¼Œä¸åŒç±»é¡¹ç›®çš„å·®å¼‚åŒ–ä¼˜åŠ¿]

### ğŸ¯ åº”ç”¨åœºæ™¯
[åˆ—ä¸¾2-3ä¸ªå…·ä½“çš„å®é™…åº”ç”¨åœºæ™¯ï¼Œè¯´æ˜è§£å†³äº†ä»€ä¹ˆé—®é¢˜]

### âš ï¸ æ½œåœ¨å±€é™
[å®¢è§‚åˆ†æé¡¹ç›®å¯èƒ½å­˜åœ¨çš„å±€é™æ€§æˆ–éœ€è¦æ³¨æ„çš„é—®é¢˜]

### ğŸ”„ ç«å“å¯¹æ¯”
[å¦‚æœæœ‰çŸ¥åç«å“ï¼Œç®€è¦å¯¹æ¯”ä¼˜åŠ£åŠ¿ï¼›å¦‚æœæ˜¯æ–°å…´é¢†åŸŸï¼Œè¯´æ˜å¸‚åœºå®šä½]

**è¦æ±‚ï¼š**
1. ä¸“ä¸šä¸”é€šä¿—æ˜“æ‡‚ï¼Œé¿å…è¿‡åº¦æŠ€æœ¯æœ¯è¯­å †ç Œ
2. å®¢è§‚ä¸­ç«‹ï¼ŒåŸºäºäº‹å®åˆ†æ
3. æ€»é•¿åº¦æ§åˆ¶åœ¨400å­—ä»¥å†…
4. çªå‡ºæŠ€æœ¯ä»·å€¼å’Œå®ç”¨æ€§
"""
        return prompt

    def _parse_repo_url(self, url: str) -> tuple:
        """è§£æGitHubä»“åº“URLï¼Œæå–ownerå’Œrepoåç§°"""
        pattern = r'github\.com/([^/]+)/([^/]+)'
        match = re.search(pattern, url)
        if match:
            return match.group(1), match.group(2)
        return None, None

    async def analyze(self, repo_data: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œå®Œæ•´çš„æ·±åº¦åˆ†ææµç¨‹"""
        repo_url = repo_data.get('url', '')

        readme_content = await self.fetch_readme(repo_url)
        dependencies = self.extract_dependencies(readme_content)
        prompt = self.build_deep_analysis_prompt(repo_data, readme_content, dependencies)

        return {
            'prompt': prompt,
            'readme_available': readme_content is not None,
            'dependencies': dependencies,
            'readme_length': len(readme_content) if readme_content else 0
        }


def create_analyzer(github_token: Optional[str] = None) -> DeepAnalyzer:
    """åˆ›å»ºæ·±åº¦åˆ†æå™¨å®ä¾‹"""
    return DeepAnalyzer(github_token)
